{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Øving 2, TMA4320\n",
    "\n",
    "* **Veiledning:** Digital. Mandag 18. januar, 1415-1600, Onsdag 20. januar, 1015-1200.\n",
    "* **Innleveringsfrist:** Onsdag 27. januar, kl 2359\n",
    "* **Innleveringsmetode** Følgende to krav er nødvendig for godkjenning\n",
    "    1. Opplasting av Jupyter Notebook (individuelt) i Blackboard\n",
    "    2. Svare på Blackboardskjema for de tre kontrollspørsmålene i øvingen\n",
    "\n",
    "\n",
    "Vi skal i denne oppgaven bruke fikspunktiterasjon og spesifikt Newtons metode, først for skalar ligning og deretter for et system av ligninger.\n",
    "\n",
    "**Oppgave 1** Fikspunktiterasjon for skalare ligninger.\n",
    "Vi ser nå på polynomet\n",
    "\n",
    "$$\n",
    "     p(x) = 3 x^6 - 6 x^2 - 5 x + 8\n",
    "$$\n",
    "\n",
    "som har 2 reelle nullpunkter, $x_1\\approx 0.973282558605377$ og $x_2=1$. Vi bruker tre omforminger av ligningen til fikspunktform $x=g(x)$ med\n",
    "\n",
    "1.  $$g(x)= \\frac{6x^2+5x-8}{3x^5}\\quad\\text{ (\"tilfeldig\")} $$\n",
    "\n",
    "2.  $$g(x)=x-\\frac{p(x)}{p'(x)}\\quad  \\text{(Newton)} $$\n",
    "\n",
    "3.  $$ g(x) = x - \\frac{p(x)}{p'(x-\\frac{p(x)}{2p'(x)})} \\quad \\text{(ny \"lur\" metode)} $$\n",
    "\n",
    "I fikspunktiterasjonen vil vi ha en startverdi $x^{(0)}$ og en toleranse $\\mathrm{tol}$. Stoppkriteriet vi bruker er\n",
    "\n",
    "$$\n",
    "    |x^{(k)}-x^{(k-1)}| < \\mathrm{tol}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Lag tre funksjoner *fiksit*, *newton* og *cuba* for tilfelle 1, 2, 3 henholdsvis. \n",
    "Inputargumenter skal for alle tre inkludere $x^{(0)}$ og toleransen tol.\n",
    "De to siste funksjonene skal ta p (funksjonen), og pd (den deriverte av p) som input, mens den første istedet tar \n",
    "den oppgitte g som input. Funksjonene skal returnere \n",
    "approksimasjon til roten samt antall iterasjoner benyttet. Sørg for at funksjonen avslutter dersom metoden ikke har konvergert etter maxiter iterasjoner, der maxiter enten kan være et inputargument eller definert inne i funksjonen. Du må også skrive funksjonene p, pd og g.\n",
    "Test ut med $x^{(0)}=1.1$ for fiksit og $x^{(0)}=1.8$ for newton og cuba, og i alle tilfeller $\\mathrm{tol}=10^{-14}$. Bestem antall iterasjoner som trengs i hver av de tre tilfellene.\n",
    "\n",
    "</div>\n",
    "\n",
    "**Kontrollspørsmål 1** Hvor mange iterasjoner trengs for konvergens av cuba på oppgitt problem, startverdi og toleranse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant \t Rot \t\t iterasjoner\n",
      "\n",
      "Fiksit:  0.9999999999999799 \t 71\n",
      "Newton:  1.0000000000000002 \t 13\n",
      "Cuba: \t 1.0 \t\t\t 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fiksit(g,x0,tol):\n",
    "    assert tol>0\n",
    "# Fyll inn kode her\n",
    "    x = x0\n",
    "    iters = 0\n",
    "    max_iters = 100\n",
    "    while np.abs(g(x)-x)>tol and iters<max_iters:\n",
    "        x = g(x)\n",
    "        iters += 1\n",
    "    return x,iters\n",
    "\n",
    "def newton(f,fd,x0,tol):\n",
    "    assert tol>0\n",
    "# Fyll inn kode her\n",
    "    x = x0\n",
    "    iters = 0\n",
    "    max_iters = 100\n",
    "    delta = f(x)/fd(x)\n",
    "    while np.abs(delta)>tol and iters<max_iters:\n",
    "        delta = f(x)/fd(x)\n",
    "        x = x - delta\n",
    "        iters += 1\n",
    "    return x,iters\n",
    "\n",
    "def cuba(f,fd,x0,tol):\n",
    "    assert tol>0\n",
    "# Fyll inn kode her\n",
    "    x = x0\n",
    "    iters = 0\n",
    "    max_iters = 100\n",
    "    delta = f(x)/fd(x-(f(x)/2/fd(x)))\n",
    "    while np.abs(delta)>tol and iters<max_iters:\n",
    "        delta = f(x)/fd(x-(f(x)/2/fd(x)))\n",
    "        x = x - delta\n",
    "        iters +=1\n",
    "    return x,iters\n",
    "\n",
    "def p(x):\n",
    "    return 3*x**6 - 6*x**2 - 5*x + 8\n",
    "def pd(x):\n",
    "    return 18*x**5 - 12*x - 5\n",
    "def g(x):\n",
    "    return (6*x**2 + 5*x - 8)/(3*x**5)\n",
    "#Gjør kall til funksjoner som utfører eksperimenter her\n",
    "\n",
    "root_1, iterations_1 = fiksit(g,1.1,10**(-14))\n",
    "root_2, iterations_2 = newton(p,pd,1.8,10**(-14))\n",
    "root_3, iterations_3 = cuba(p,pd,1.8,10**(-14))\n",
    "\n",
    "print(\"Variant \\t Rot \\t\\t iterasjoner\\n\")\n",
    "print(\"Fiksit: \", root_1,'\\t', iterations_1)\n",
    "print(\"Newton: \", root_2,'\\t', iterations_2)\n",
    "print(\"Cuba: \\t\", root_3,'\\t\\t\\t', iterations_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 2.** Vi skal nå se på bruk av automatisk derivasjon for Newtons metode. Automatisk derivasjon er en teknikk som ble oppfunnet for ikke så veldig mange år siden. Det er en algoritme som kan ta en vilkårlig funksjon skrevet f.eks. i Python, og genererer fra denne automatisk den deriverte av funksjonen. Det er ikke det samme som symbolsk derivasjon for resultatet foreligger ikke som et symbolsk uttrykk. I dette kurset skal vi kun bruke ferdig implementert automatisk derivasjon. Det fins en pakke for dette formålet i Python som heter *autograd*. Vi kan ikke benytte 'vanlig numpy' når vi gjør autoderivasjon, men bruker en variant autograd.numpy, det er derfor denne vi nå må importere. I vinduet nedenfor demonstrerer vi bruken og har valgt å hente inn funksjonen *value_and_grad* fra *autograd*. Den kan brukes til å finne et tuple bestående av funksjonsverdi og derivert for gitte verdier av argumentet til funksjonen. Mye blir nok klart hvis du leser følgende eksempel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49999999999999994\n",
      "0.8660254037844387\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import value_and_grad\n",
    "\n",
    "def g(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "Dg = value_and_grad(g)\n",
    "G=Dg(np.pi/6.)\n",
    "print(G[0])\n",
    "print(G[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med $g(x)=\\sin x$ får vi laget funksjonen Dg som returnerer tuplet $(g(x),g'(x))=(\\sin x, \\cos x)$.\n",
    "Som du vet er $\\sin\\frac{\\pi}{6}=\\frac12$ mens $\\cos\\frac{\\pi}{6}=\\frac12\\sqrt{3}\\approx 0.866$.\n",
    "\n",
    "Når man bruker *autograd* er datatypen til input viktig. Du bør for eksempel alltid definere tall som skal være flyttall med desimalpunktum, f.eks. skriv ikke *x0=1*, men *x0=1.0*. I motsatt fall kan du få kryptiske feilmeldinger.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(a)** Lag en funksjon *gNewton* som tar tre inputargumenter: funksjonen $f(x)$ som en skal finne nullpunkter til, startverdien $x^{(0)}$, og toleransen tol. Returner approksimert rot og antall iterasjoner. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(b)** Test *gNewton* på funksjonen\n",
    "\n",
    "$$\n",
    " f(x) = \\frac13\\sqrt{1+\\sin(\\tanh x)}+\\cos x\n",
    "$$\n",
    "med startverdi $x0=\\{-2.0,2.0,4.0,8.0\\}$, $\\mathrm{tol}=10^{-10}$ og skriv ut rotapproksimasjon og antall iterasjoner i hvert tilfelle. \n",
    "\n",
    "</div>\n",
    "\n",
    "**Kontrollspørsmål 2** Hvor mange distinkte (forskjellige) røtter fikk du konvergens mot totalt med disse 4 startverdiene?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant \t\t\t Rot \t\t iterasjoner\n",
      "\n",
      "1.7180861141125476\n",
      "gNewton with x0 = -2.0:  -1.7180861141125476 \t 4\n",
      "2.0376244457683277\n",
      "gNewton with x0 = 2.0:  2.0376244457683277 \t 4\n",
      "4.24303676441417\n",
      "gNewton with x0 = 4.0:  4.24303676441417 \t 5\n",
      "8.323364548870096\n",
      "gNewton with x0 = 8.0:  8.323364548870096 \t 5\n"
     ]
    }
   ],
   "source": [
    "# Fyll inn koden for b-spørsmålet her\n",
    "import autograd.numpy as np\n",
    "from autograd import value_and_grad\n",
    "\n",
    "def f(x):\n",
    "    # Fyll inn definisjon av den konkrete funksjonen f her\n",
    "    return 1/3*np.sqrt(1+np.sin(np.tanh(x))) + np.cos(x)\n",
    "def test(x):\n",
    "    return x**2 +5*x -10000\n",
    "\n",
    "\n",
    "def gNewton(f, x0, tol):\n",
    "    assert tol>0\n",
    "    iters = 0\n",
    "    max_iters = 50\n",
    "    Df = value_and_grad(f)\n",
    "    x = x0\n",
    "    F = Df(x)\n",
    "    delta = F[0]/F[1]\n",
    "    while np.abs(delta)>tol and iters<max_iters:\n",
    "        F = Df(x)\n",
    "        delta = F[0]/F[1]\n",
    "        x = x-delta\n",
    "        iters += 1\n",
    "    print(np.abs(f(x)-x))\n",
    "    return x, iters\n",
    "# Fyll in kode for gNewton her\n",
    "\n",
    "# Utfør påkrevde tester under her\n",
    "x_0 = (-2.0,2.0,4.0,8.0)\n",
    "print(\"Variant \\t\\t\\t Rot \\t\\t iterasjoner\\n\")\n",
    "for x in x_0:\n",
    "    root, iterations = gNewton(f,x,10**(-10))\n",
    "    print(f\"gNewton with x0 = {x}: \", root,'\\t', iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 3** Newtons metode for systemer av ligninger\n",
    "\n",
    "Nå skal vi se på systemer av ligninger.\n",
    "Newton's metode kan fremdeles defineres, men må generaliseres. Et $n$-dimensjonalt system av ligninger kan skrives\n",
    "$$\n",
    "\\begin{array}{lcr}\n",
    "f_0(x_0,\\ldots,x_{n-1})&=&0\\\\\n",
    "f_1(x_0,\\ldots,x_{n-1})&=&0\\\\\n",
    "&\\vdots&\\\\\n",
    "f_{n-1}(x_0,\\ldots,x_{n-1})&=&0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "dvs $n$ ligninger $f_0,\\ldots,f_{n-1}$ for $n$ ukjente $x_0,\\ldots,x_{n-1}$. \n",
    "I kortspråk skriver vi dette kun som $\\mathbf{f}(\\mathbf{x})=0$.\n",
    "\n",
    "En sentral størrelse her er den såkalte\n",
    "Jacobi-matrisen. Hvis vi partiellderiverer $n$ funksjoner, hver med hensyn på $n$ variable, så blir det totalt\n",
    "$n^2$ funksjoner, dette er elementene i Jacobimatrisen\n",
    "\n",
    "$$\n",
    "D\\mathbf{f} = \\left[\n",
    "\\begin{array}{cccc}\n",
    "\\frac{\\partial f_0}{\\partial x_0} & \\frac{\\partial f_0}{\\partial x_1} & \\cdots & \\frac{\\partial f_0}{\\partial x_{n-1}} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_0} & \\frac{\\partial f_1}{\\partial x_1} &\\cdots & \\frac{\\partial f_1}{\\partial x_{n-1}}\\\\\n",
    "\\vdots                            & \\vdots                            &        &    \\vdots   \\\\\n",
    "\\frac{\\partial f_{n-1}}{\\partial x_0} & \\frac{\\partial f_{n-1}}{\\partial x_1} &\\cdots & \\frac{\\partial f_{n-1}}{\\partial x_{n-1}}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Generaliseringen av Newton's metode er nå rett og slett: Gitt $\\mathbf{x}^{(0)}\\in\\mathbb{R}^n$ la\n",
    "\n",
    "$$\n",
    "    \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\left[D\\mathbf{f}(\\mathbf{x}^{(k)})\\right]^{-1}\\cdot \\mathbf{f}(\\mathbf{x}^{(k)})\n",
    "$$\n",
    "\n",
    "For å implementere denne kan vi definere $\\mathbf{f}$ som en funksjon som tar et numpy-array som input og returnerer et numpy-array. Så fins det en funksjon i pakken *autograd* som heter *jacobian* og som beregner Jacobimatrisen. Vi illustrerer hvordan vi bruker *jacobian* på funksjonen \n",
    "\n",
    "$$\n",
    "\\mathbf{f}(x_0,x_1) = \\left[\\begin{array}{c}x_0^2+x_1^2\\\\ x_0 x_1\\end{array}\\right]\\quad\\Rightarrow\\quad \n",
    "Df=\\left[\n",
    "\\begin{array}{cc}\n",
    "2x_0 & 2 x_1 \\\\ x_1 & x_0\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 4.]\n",
      " [2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "\n",
    "def f(x):\n",
    "    return np.array([x[0]**2+x[1]**2,x[0]*x[1]])\n",
    "\n",
    "Df = jacobian(f)\n",
    "x=np.array([1.0,2.0])\n",
    "print(Df(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk at kallet til *jacobian* gjøres bare én gang når *f* er definert. Etterpå kan *Df* brukes flere ganger med ulike inputargumenter.\n",
    "\n",
    "Nå litt om hvordan man implementerer Newton's metode. Inversmatrisen som figurere i iterasjonen gjør vi her enkelt ved å kalle en ferdigrutine i *numpy.linalg*, se nedenfor. Hvis vi definerer $\\delta=D\\mathbf{f}(\\mathbf{x}^{(k)})^{-1}\\cdot\\mathbf{f}(\\mathbf{x}^{(k)})$ kan vi skrive en Newtoniterasjon som\n",
    "1. Løs $D\\mathbf{f}(\\mathbf{x}^{(k)})\\delta = -\\mathbf{f}(\\mathbf{x}^{(k)})$ m.h.p. $\\delta$\n",
    "2. Sett $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\delta$\n",
    "\n",
    "I *numpy* fins en delpakke som heter *numpy.linalg* og i denne fins en funksjon som heter *solve*. Derfor kan vi løse ligningssystemer som i 1. ved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "A=np.array([[2,1],[3,1]])\n",
    "b=np.array([1,1])\n",
    "y=la.solve(A,b)\n",
    "print(y,la.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har også tatt med *la.norm* som beregner lengden til en vektor. Det trenger vi å gjøre når vi skal lage stoppkriterium i Newtoniterasjonen, det er naturlig å kreve at $\\mathrm{la.norm(delta)}<\\mathrm{tol}$.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(a)** Skriv en funksjon sNewton som tar tre inputargumenter: funksjonen $\\mathbf{f}(\\mathbf{x})$ som en skal finne nullpunkter til, startverdien $\\mathbf{x}^{(0)}$, og toleransen tol. Returner approksimert rot og antall iterasjoner.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(b)** Test ut sNewton på å finne nullpunkter til systemet\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "x_0^2 + x_1^2 &=& 1 \\\\\n",
    "x_0^3 - x_1 &=& 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "idet du velger toleransen slik at du kan gi et pålitelig svar på Kontrollspørsmål 3. Den første ligningen sier at eventuelle nullpunkter ligger på sirkelen sentrert i $(0,0)$ med radius 1. Den kubiske kurven $x_1=x_0^3$ fra den andre ligningen skjærer denne sirkelen nøyaktig to steder, så det fins akkurat 2 nullpunkter.\n",
    "</div>\n",
    "\n",
    "**Kontrollspørsmål 3.** I spørsmål **(3b)** angi det åttende desimalet etter komma for hhv $x_0$- og $x_1$-komponenten av roten (merk at det spiller ingen rolle hvilken av de to røttene du velger). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant \t Rot \t\t iterasjoner\n",
      "\n",
      "sNewton:  0.826031358 ,  0.563624162 \t 8\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import numpy.linalg as la\n",
    "\n",
    "def f(x):\n",
    "    f1 = x[0]**2.0+x[1]**2.0-1.0\n",
    "    f2 = x[0]**3-x[1]\n",
    "    return np.array([f1,f2])\n",
    "\n",
    "def sNewton(f,x0,tol):\n",
    "    #Skriv kode for Newtoniterasjoner for systemer med automatisk derivasjon her\n",
    "    Df = jacobian(f)\n",
    "    iters = 0\n",
    "    max_iters = 100\n",
    "    x = x0\n",
    "    F = Df(x)\n",
    "    delta = la.inv(F)@f(x)\n",
    "    while la.norm(delta)>tol and iters<max_iters:\n",
    "        F = Df(x)\n",
    "        delta = la.inv(F)@f(x)\n",
    "        x = x - delta\n",
    "        iters += 1\n",
    "    return x, iters\n",
    "        \n",
    "\n",
    "#Gjør kall til sNewton og utfør påkrevde eksperimenter under her \n",
    "root, iterations = sNewton(f,np.array([3.0,3.0]), tol = 10**(-10))\n",
    "\n",
    "print(\"Variant \\t Rot \\t\\t iterasjoner\\n\")\n",
    "print(\"sNewton: \", f'{root[0]:.9f}',', ',f'{root[1]:.9f}','\\t', iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
