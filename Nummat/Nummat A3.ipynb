{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMA4215 Numerisk Matematikk \n",
    "\n",
    "Høst 2021 – September 24, 2021\n",
    "\n",
    "R. Bergmann, E. Çokaj, O. P. Hellan \n",
    "\n",
    "# Assignment 3\n",
    "\n",
    "## Deadline\n",
    "October 1, 2021, 23:59\n",
    "\n",
    "\n",
    "## Submission\n",
    "submit your Jupyter notebook containing the solution via upload in blackboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "1. Consider the Problem\n",
    "   \\begin{equation*}\n",
    "\t   \\operatorname*{arg\\,min}_{\\mathbf x\\in \\mathbb R^2}\n",
    "       x_1^2 + (x_2-3)^2.\n",
    "   \\end{equation*}\n",
    "   Compute two steps of the gradient method with exact line search starting from $\\mathbf x^{(0)} = (0,0)^{\\mathrm{T}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the problem do not specify, I chose the $\\alpha = 1$\n",
    "$$\n",
    "\\nabla F = [2 x_1,  2 x_2 - 6]\n",
    "$$\n",
    "$$\n",
    "d = \\nabla F(0,0) = [0,-6]/6 = [0,-1]\n",
    "$$\n",
    "$$\n",
    "x^{(1)} = x^{(0)} + \\alpha d = (0,-\\alpha)\n",
    "$$\n",
    "Then iteration 2 gives:\n",
    "$$\n",
    "d = \\nabla F(0,-\\alpha) = [0,-(6 + 2\\alpha)]/((6 + 2\\alpha) = [0,-1]\n",
    "$$\n",
    "$$\n",
    "x^{(2)} = x^{(1)} + \\alpha d = (0,-2\\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla F = \\begin{pmatrix}2x_1 \\\\2x_2 -6 \\end{pmatrix} \\\\\n",
    "H_F = \\begin{pmatrix}  2 & 0 \\\\ 0 & 2 \\end{pmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as smp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x = smp.symbols(\"x_1 x_2 x\")\n",
    "F = x1**2 + (x2 - 3)**2\n",
    "f1 = smp.diff(F, x1)\n",
    "f2 = smp.diff(F, x2)\n",
    "f = smp.Matrix([f1, f2])\n",
    "h11 = smp.diff(f1, x1)\n",
    "h12 = smp.diff(f2, x1) \n",
    "h21 = smp.diff(f1, x2) \n",
    "h22 = smp.diff(f2, x2)\n",
    "H = smp.Matrix([[h11, h12], [h21, h22]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2*x_1: 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq1 = h11*x1 + h12*x2\n",
    "eq2 = h21*x1 + h22*x2\n",
    "d = smp.solve((eq1, eq2), (f1,f2))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Let $A\\in \\mathbb R^{n\\times n}$, $\\mathbf b\\in\\mathbb R^{n}$, and $\\lambda > 0$ be given.\n",
    "\n",
    "Assume we want to avoid too large solutions. Then we can look at the (so-called) Thikonov regularisation. Instead of solving $A\\mathbf x = \\mathbf b$ we solve\n",
    "\n",
    "$$\n",
    "F(\\mathbf x) = \\lVert A\\mathbf x - \\mathbf b\\rVert_2^2 + \\lambda\\lVert \\mathbf x \\rVert_2^2\n",
    "$$\n",
    "\n",
    "1. Show that $F$ has a unique solution by computing the gradient and the Hessian of $F$, using the gradient to obtain the solution and arguing that the Hessian is symmetric positive definte. \n",
    "2. Develop the gradient descent method for $F$ for given step sizes $\\alpha_k$ (i.e. they are given by the user).\n",
    "3. _Bonus question_: Derive a closed form solution for exact line search for $\\alpha_k$ in the gradient descent, i.e. at $\\mathbf x^{(k)}$ with step $\\mathbf d^{(k)}$, find $\\alpha_k$ that minimizes $\\phi(\\alpha) = F(\\mathbf x^{(k)} + \\alpha\\mathbf d^{(k)})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2a)\n",
    "<!-- $$\n",
    "\\nabla F = \\frac{\\partial}{\\partial x} (Ax)^T Ax +  (Ax)^T b + b^T Ax + b^T b + \\lambda \\lVert x \\rVert_2^2\n",
    "= (Ax)^T A+ A^TAx +  A^T b + b^T A + \\lambda 2x\n",
    "= x^T A^TA\\mathbf{1} + \\mathbf{1}^TA^TAx + \\mathbf{1}^TA^Tb + b^T A \\mathbf{1} + \\lambda 2x\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla F = \\frac{\\partial}{\\partial x} <(Ax-b),(Ax-b)> + \\lambda \\lVert x \\rVert_2^2 = \n",
    "2 A^TAx - 2Ab + 2 \\lambda x =\n",
    "2 (A^TA +2 \\lambda I)x - 2Ab\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_F = 2A^TA + 2 \\lambda I\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradiant = 0 gives the system of equations $$ 2 (A^TA +2 \\lambda I)x = 2Ab $$\n",
    "This is guarantied to have an unique solution if H_F is positive definite on the whole space, which it is as a matrix $A^TA$ have all semi positive entries and is therefore itself semipositive definite. Adding a $\\lambda I > 0$ then assures the hessian is positive deinite and not only semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using -gradiant F as descent , we iterate with $$ x^{(k+1)} = x^{(k)} + \\alpha_k d$$ where $d$ is the normalized descent, and $\\alpha_k$ some stepsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- $$\n",
    "\\nabla F = \\frac{\\partial}{\\partial x} (Ax)^T Ax +  (Ax)^T b + b^T Ax + b^T b + \\lambda \\lVert x \\rVert_2^2\n",
    "= \\frac{Ax - b}{\\lVert Ax- b \\rVert_2^2} + \\lambda 2x\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Implement the gradient method with an Armijo step size strategy for a multivariate function $F: \\mathbb R^{n} \\to \\mathbb R$.\n",
    "\n",
    "1. Implement a function\n",
    "\n",
    "   ```\n",
    "   linesearch_armijo(f, gradf, xk, dk, a0, c, s)\n",
    "   ```\n",
    "\n",
    "    that looks for a function value starting with $\\mathbf x + a_0 \\mathbf d$, decreasing $a_j = c^ja_0$ (obviously $0<c<1$), ... until $f(\\mathbf x) - f(\\mathbf x + \\alpha_j\\mathbf d) > -\\alpha_j s (\\nabla f(\\mathbf x)^{\\mathrm{T}}\\mathbf d)$ for the input parameter $0<s<1$.\n",
    "\n",
    "2. Implement a function\n",
    "\n",
    "   ```\n",
    "   steepest_descent(f, gradf, x0, a0, c, s, xtol, ftol, maxiter)\n",
    "   ```\n",
    "\n",
    "   where $a0,c,s$ are parameters for the (inner) Armijo line search,\n",
    "   and the stopping criterion should be a combination of a maximal number of iterations (`maxiter`), a tolerance in the _change_ of the iterates $\\mathbf x^{(k)}$ and a tolerance in the change of the function values. If one of the tolerances is met (i.e. we are _below_) the algorithm should stop.\n",
    "   Set reasonable default values for these values. For Armijo, a reasonable choice is `a0=1; c=0.5; s=10e-2`.\n",
    "   As a return value we would like to have the vector of all iterates `[x0, x1,...]`.\n",
    "\n",
    "3. For the test, please also visualise the iterates in either a plot of the function (1D) or a contour plot of the function (2D).<br>\n",
    "   a. $f(x) = \\cos(x)$ with $x^{(0)}=1.1656$,<br>\n",
    "   b. Himmelblau's function $F\\colon\\mathbb R^2 \\to \\mathbb R$<br>\n",
    "      $$\n",
    "      F(\\mathbf x) = \\bigl( x_1^2+x_2-11 \\bigr)^2 + \\bigl( x_1+x_2^2-7 \\bigr)^2\n",
    "      $$\n",
    "    with different start values\n",
    "    $\\mathbf x_a^{(0)} = (-0.27, -0.91)^{\\mathrm{T}}$, $\\mathbf x_b^{(0)} = (-0.271, -0.91)^{\\mathrm{T}}$, $\\mathbf x_c^{(0)} = (-0.25 , -1.1)^{\\mathrm{T}}$, and $\\mathbf x_d^{(0)} = (-0.25, -1)^{\\mathrm{T}}$. Visualize all 4 runs in one contour plot.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
