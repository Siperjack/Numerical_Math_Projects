{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7730905c",
   "metadata": {},
   "source": [
    "### TMA4215 Numerisk Matematikk \n",
    "\n",
    "Høst 2021 – September 10, 2021\n",
    "\n",
    "R. Bergmann, E. Çokaj, O. P. Hellan \n",
    "\n",
    "# Assignment 2\n",
    "\n",
    "## Deadline\n",
    "September 17, 2021, 23:59\n",
    "\n",
    "\n",
    "## Submission\n",
    "submit your Jupyter notebook containing the solution via upload in blackboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ca758",
   "metadata": {},
   "source": [
    "#####  Libraries used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "43d9e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as smp\n",
    "import numpy as np\n",
    "from sympy import *\n",
    "np.set_printoptions(precision=3)\n",
    "from datetime import datetime\n",
    "import scipy.linalg as lng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38bdca",
   "metadata": {},
   "source": [
    "##### Earlier implemented funcitons used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bbc3e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used in gaussedel implementation\n",
    "def forward_subtitution(A,b):\n",
    "    N = len(b)\n",
    "    c = np.copy(b)\n",
    "    A = A.copy()\n",
    "    x = np.zeros(len(b))\n",
    "    x[0] = c[0]\n",
    "    for i in range(1,N):\n",
    "        sums = 0\n",
    "        for j in range(0,i):\n",
    "            sums += A[i][j]*x[j]\n",
    "        x[i] = (c[i] - sums)/A[i][i]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aaa301",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Let $A\\in \\mathbb R^{n\\times n}$ denote an invertible $n$-by-$n$ dimensional matrix.\n",
    "\n",
    "Compute the following gradients $\\nabla f$ and Jacobi matrices $J_F$, respectively. At which points is the gradient or Jacobi matrix welldefined? \n",
    "1. $f_1\\colon \\mathbb R^n \\to \\mathbb R$ with $f_1(\\mathbf{x}) = \\lVert A\\mathbf{x} \\rVert_2^2$ \n",
    "2. $f_2\\colon \\mathbb R^n \\to \\mathbb R$ with $f_2(\\mathbf{x}) = \\lVert \\mathbf x \\rVert$\n",
    "3. $f_3\\colon \\mathbb R^n \\to \\mathbb R$ with $f_3(\\mathbf{x}) = (A\\mathbf{x}, \\mathbf{x})$, where $(\\cdot,\\cdot)$ denotes the usual inner product. How does this simplify if $A$ is symmetric?\n",
    "4. $F_4\\colon \\mathbb R^n \\to \\mathbb R^n$ with $F_4(\\mathbf{x}) = A\\mathbf{x}$\n",
    "\n",
    "Further compute the Hessian Matrix $\\nabla^2 f$ of the function $f_3$.\n",
    "\n",
    "_Bonus question_: Compute the gradient for $F_5\\colon\\mathbb R^n\\to\\mathbb R$, $F_5(\\mathbf x) = \\lVert A\\mathbf x - \\mathbf b\\rVert_2^2$ for a rectangular matrix $A\\in\\mathbb R^{m\\times n}$, $m\\geq n$, and a vector $\\mathbf b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee886a",
   "metadata": {},
   "source": [
    "##### Solutions problem 1:\n",
    "1) $$ \\nabla_k f = \\frac{\\partial}{\\partial x_k} \\sum_i \\lvert \\sum_j  a_{ij} x_j\\rvert^2 = \\frac{\\partial}{\\partial x_k} \\sum_i ( \\sum_j  a_{ij} x_j)^2 = 2 \\sum_i ( \\sum_j  a_{ij} x_j)a_{ii}  = \n",
    "2 \\sum_j a_{ii}a_{ij} x_j $$\n",
    "\n",
    "This gradient is well defined for all $\\mathbf{x}$\n",
    "\n",
    "2) For $x > 0$ $$\\nabla_k f = +1$$\n",
    "and for $x < 0$ $$\\nabla_k f = -1$$\n",
    "\n",
    "This gradient is well defined for all $\\mathbf{x}$ which have no component $x_i = 0$.\n",
    "\n",
    "3) $$ \\nabla_k f= \\frac{\\partial}{\\partial x_k} (A\\mathbf{x}, \\mathbf{x}) = \\frac{\\partial}{\\partial x_k} \\sum_i \\sum_j a_{ij} x_j x_i = \\sum_ i  a_{ik} x_i + \\sum_{j} a_{kj} x_j + a_{kk} x_k\n",
    "$$\n",
    "\n",
    "This gradient is well defined for all $\\mathbf{x}$\n",
    "\n",
    "$$ H_{kl}(f) =  \\frac{\\partial}{\\partial x_k} \\nabla_l f = \\frac{\\partial}{\\partial x_l} (\\sum_ i  a_{ik} x_i + \\sum_{j} a_{kj} x_j + a_{kk} x_k) = \\begin{cases} a_{lk} + a_{kl} & \\text{ for k} \\neq l \\\\ 3 a_{kk} & \\text{ for k = l}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "4) $$ J(F) = J(Ax) = A $$\n",
    "\n",
    "This jacobian is well defined for all $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcae22a",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "We consider the linear system of equations\n",
    "$A\\mathbf{x}=\\mathbf{b}$, where\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "   3 & -1 & -c\\\\\n",
    "  -1 &  3 &  0\\\\\n",
    "  -c &  0 &  3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "for some $c\\in \\mathbb R$.\n",
    "\n",
    "1. Consider the Jacobi Method. What does the iteration matrix $B_{\\mathrm{J}}$ look like?\n",
    "2. For which parameters $c \\in \\mathbb R$ does the Jacobi method converge?\n",
    "    \n",
    "    _Hint:_ Consider the eigenvalues of $B_{\\mathrm{J}}$.\n",
    "3. Prove that the Gauss-Seidel Method converges for any $c \\in [-1,1]$.  \n",
    "    \n",
    "    _Hint:_ you may use the property, that the Gauss-Seidel method converges if $A$ is strictly diagonal dominant, i.e. if\n",
    "      $\\lvert a_ii \\rvert > \\displaystyle\\sum_{i\\neq j} \\lvert a_{ij} \\rvert$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210d628",
   "metadata": {},
   "source": [
    "### Solutions problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a342ae",
   "metadata": {},
   "source": [
    "##### 2.1)\n",
    "$B_J$ is for the Jacobi method is the $P^{-1} N$, where $P$ is the preconditioning matrix which for the Jacobi method is the diagonal $D$ of $A$, and $N = A - D$. $$P^{-1} N = \\begin{pmatrix}\n",
    "   1/3 & 0 & 0\\\\\n",
    "  0 &  1/3 &  0\\\\\n",
    "  0 &  0 &  1/3\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "   0 & -1 & -c\\\\\n",
    "  -1 &  0 &  0\\\\\n",
    "  -c &  0 &  0\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}0&-\\frac{1}{3}&-\\frac{c}{3}\\\\ -\\frac{1}{3}&0&0\\\\ -\\frac{c}{3}&0&0\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b486b",
   "metadata": {},
   "source": [
    "##### 2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "abeff064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Lambda: 0.0},\n",
       " {Lambda: -0.333333333333333*sqrt(c**2 + 1.0)},\n",
       " {Lambda: 0.333333333333333*sqrt(c**2 + 1.0)}]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= smp.symbols(\"c\")\n",
    "c, Lambda = symbols('c Lambda')\n",
    "I = eye(3)\n",
    "A = Matrix([[0, -1/3, -c/3], [-1/3, 0, 0], [-c/3, 0, 0]])\n",
    "equation = Eq(det(Lambda*I-A), 0)\n",
    "D = solve(equation)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ce2fe",
   "metadata": {},
   "source": [
    "Ser at så lenge $c < sqrt(8) $ så vil specralradien $\\rho(B_J) < 1$, og som et resultat vil iterasjonen konvergere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058a794",
   "metadata": {},
   "source": [
    "##### 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf7705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edad43c0",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "Consider the matrix $A\\in \\mathbb R^{n\\times n}$ of the form $a_{ij} = 3^{-\\lvert i-j\\rvert} + 2^{-i-j} + 10^{-6} \\cdot$ `randn()`, where `randn()` refers to a random number per entry (independently) normal distributed with mean `0` and variance `1`, cf. `numpy.random.randn`. Further let $\\mathbf{b} = (1,\\ldots,1)^\\mathrm{T} \\in \\mathbb R^{n}$.\n",
    "\n",
    "We want to compare a few algorithms. Implement one method\n",
    "\n",
    "```\n",
    "    method_name(A,b,x0,tol,maxiter)\n",
    "```\n",
    "\n",
    "For each of the following three or four methods (the fourth is optional), where the `method_name` is given in brackets, but they should all follow the same interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823321a",
   "metadata": {},
   "source": [
    "__Richardson iteration__ (`richardson(...)`)\n",
    "\n",
    "Repeat for \\(k=0,...\\) until the stopping criterion is fulfilled\n",
    "\n",
    "$$\\begin{split}\n",
    "\\mathbf{r}^{(k)} &= \\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(k)},\\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\omega \\mathbf{r^{(k)}}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where $\\omega$ is a sixth (optional) parameter to the function `richardson()`.\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "48d21fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def richardson(A,b,x0,tol,maxiter,w = 1):\n",
    "    count = 0\n",
    "    r0 = b - A@x0\n",
    "    r = r0\n",
    "    error = 1\n",
    "    x = x0\n",
    "    \n",
    "    #Tracking time\n",
    "    current_time1 = datetime.now()\n",
    "\n",
    "    while error > tol: #norm = the standard 2-norm\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        ################## All but this block are identical across the methods ##########################\n",
    "        \n",
    "        \n",
    "        r = b - A@x\n",
    "        x_new = x + w*r\n",
    "        \n",
    "       \n",
    "        ################# End of uniqueness #####################\n",
    "        x = x_new\n",
    "        error = np.linalg.norm(r)/np.linalg.norm(r0)\n",
    "        if count > maxiter:\n",
    "            print(\"Error: Tollerance not reached before maxiter\")\n",
    "            return\n",
    "    print(\"Solution with error under tolerance reached \\n\")\n",
    "    \n",
    "    current_time2 = datetime.now()\n",
    "    \n",
    "    print(\"Time elapsed =\", current_time2-current_time1)\n",
    "    print(f\"Final error = {error:.2e} \\nNumber of iterations = {count}\")\n",
    "    \n",
    "    return x, error, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5091dd1",
   "metadata": {},
   "source": [
    "__Gauß-Seidel Method__ (`gaussseidel(...)`)\n",
    "\n",
    "Repeat for \\(k=0,...\\) until the stopping criterion is fulfilled\n",
    "\n",
    "$$x_j^{(k+1)} = \\frac{1}{a_{jj}}\\left(b_j -\\sum_{i=1}^{j-1}a_{ji}x_i^{(k+1)}-\\sum_{i=j+1}^n a_{ji}x_i^{(k)}\\right),\\quad j=1,\\ldots,n.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "622f0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gausseidel(A,b,x0,tol,maxiter):\n",
    "    count = 0\n",
    "    r0 = b - A@x0\n",
    "    r = r0\n",
    "    error = 1\n",
    "    x = x0\n",
    "    \n",
    "    #Tracking time\n",
    "    current_time1 = datetime.now()\n",
    "    \n",
    "    #Additive splitting\n",
    "    F = np.triu(A,1)\n",
    "    N = F\n",
    "    P = A + N\n",
    "    while error > tol: #norm = the standard 2-norm\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        ################## All but block are identical across the methods (exept the additive split done over for this one)\n",
    "        \n",
    "        r = b - A@x\n",
    "        x_new = x + lng.solve_triangular(P,r)\n",
    "        \n",
    "       \n",
    "        ################# End of uniqueness #####################\n",
    "        x = x_new\n",
    "        error = np.linalg.norm(r)/np.linalg.norm(r0)\n",
    "        if count > maxiter:\n",
    "            print(\"Error: Tollerance not reached before maxiter\")\n",
    "            return\n",
    "    print(\"Solution with error under tolerance reached \\n\")\n",
    "    \n",
    "    current_time2 = datetime.now()\n",
    "    print(\"Time elapsed =\", current_time2-current_time1)\n",
    "    print(f\"Final error = {error:.2e} \\nNumber of iterations = {count}\")\n",
    "    \n",
    "    return x, error, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0ee805a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = np.full((5,5),1)\n",
    "# np.triu(A,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03b798",
   "metadata": {},
   "source": [
    "__Gradient Descent__ (`steepestdescent(...)`)\n",
    "\n",
    "Initialize\n",
    "\n",
    "$$\\mathbf{r}^{(0)} =\t\\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(0)}$$\n",
    "\n",
    "Repeat for \\(k=0,...\\) until the stopping criterion is fulfilled\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\t\t\\omega_k\n",
    "\t\t&=\n",
    "\t\t\\frac{(\\mathbf{r}^{(k)})^\\mathrm{T}\\mathbf{r}^{(k)}}{(\\mathbf{r}^{(k)})^\\mathrm{T}\\mathbf{A}\\mathbf{r}^{(k)}},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{x}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{x}^{(k)} + \\omega_k\\mathbf{r}^{(k)},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{r}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{r}^{(k)} - \\omega_k\\mathbf{A}\\mathbf{r}^{(k)}.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f98aedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepestdescent(A,b,x0,tol,maxiter):\n",
    "    count = 0\n",
    "    r0 = b - A@x0\n",
    "    r = r0\n",
    "    error = 1\n",
    "    x = x0\n",
    "    \n",
    "    #Tracking time\n",
    "    current_time1 = datetime.now()\n",
    "\n",
    "    while error > tol: #norm = the standard 2-norm\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        ################## All but this block are identical across the methods ##########################\n",
    "        \n",
    "        \n",
    "        w = np.transpose(r) @ r / (np.transpose(r) @ A @ r)\n",
    "        x_new = x + w*r\n",
    "        r = r - w*A@r\n",
    "        \n",
    "       \n",
    "        ################# End of uniqueness #####################\n",
    "        x = x_new\n",
    "        error = np.linalg.norm(r)/np.linalg.norm(r0)\n",
    "        if count > maxiter:\n",
    "            print(\"Error: Tollerance not reached before maxiter\")\n",
    "            return\n",
    "    print(\"Solution with error under tolerance reached \\n\")\n",
    "    \n",
    "    current_time2 = datetime.now()\n",
    "    print(\"Time elapsed =\", current_time2-current_time1)\n",
    "    print(f\"Final error = {error:.2e} \\nNumber of iterations = {count}\")\n",
    "    \n",
    "    return x, error, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa723075",
   "metadata": {},
   "source": [
    "__Conjugate Gradient Iteration__ (`cg(...)`) (_Bonus Algorithm_)\n",
    "\n",
    "Initialize\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{v}^{(0)}\n",
    "=\n",
    "\\mathbf{r}^{(0)}\n",
    "&=\n",
    "\\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(0)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Repeat for \\(k=0,...\\) until the stopping criterion is fulfilled\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\omega_k &=\n",
    "\t\t\\frac{(\\mathbf{r}^{(k)})^\\mathrm{T}\\mathbf{r}^{(k)}}{(\\mathbf{v}^{(k)})^\\mathrm{T}\\mathbf{A}\\mathbf{v}^{(k)}},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{x}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{x}^{(k)} + \\omega_k\\mathbf{v}^{(k)},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{r}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{r}^{(k)} - \\omega_k\\mathbf{A}\\mathbf{v}^{(k)},\n",
    "\t\t\\\\\n",
    "\t\t\\beta_k\n",
    "\t\t&=\n",
    "\t\t\\frac{(\\mathbf{r}^{(k+1)})^\\mathrm{T}\\mathbf{r}^{(k+1)}}{(\\mathbf{r}^{(k)})^\\mathrm{T}\\mathbf{r}^{(k)}},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{v}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{r}^{(k+1)} + \\beta_k\\mathbf{v}^{(k)}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "All should stop if _either_ the relative error $\\frac{\\lVert \\mathbf{r}^{(k)} \\rVert}{\\lVert \\mathbf{r}^{(0)} \\rVert} < \\varepsilon$, where $\\varepsilon$ is our `tol`erance  _or_ a maximum number of iterattions `maxiter` is reached; in your experiments, use \n",
    "\n",
    "* matrix size `n=2000`\n",
    "* `tol=10^{-7}`\n",
    "* `maxiter=5000`.\n",
    "\n",
    "We always start with $\\mathbf{x}^{(0)} = \\mathbf b$.\n",
    "\n",
    "We are interested in a comparison of __runtime__, __number of iterations__ and __final error__ for each of the four methods. Either provide this automatically with your code or provide the results in a Markdown table (copied from your measurements in the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "57245316",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementing A\n",
    "\n",
    "def A_generate(n):\n",
    "    A = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            A[i,j] = 3**(-abs(i-j)) + 2**(-i-j) + 10**(-6)*np.random.rand()\n",
    "    return A\n",
    "\n",
    "n = 2000\n",
    "A = A_generate(n)\n",
    "b = np.full(n,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "87b8d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = 5000\n",
    "tol = 1E-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f67a5312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution with error under tolerance reached \n",
      "\n",
      "Time elapsed = 0:00:00.407168\n",
      "Final error = 9.64e-08 \n",
      "Number of iterations = 159\n"
     ]
    }
   ],
   "source": [
    "# Richardson \n",
    "xsol1, error1, count1 = richardson(A,b,b,tol,maxiter, w = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6ae6fd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution with error under tolerance reached \n",
      "\n",
      "Time elapsed = 0:00:00.342056\n",
      "Final error = 9.44e-08 \n",
      "Number of iterations = 30\n"
     ]
    }
   ],
   "source": [
    "# Gauss-Seidel \n",
    "xsol2, error2, count2 = gausseidel(A,b,b,tol,maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "df0ad592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution with error under tolerance reached \n",
      "\n",
      "Time elapsed = 0:00:00.547043\n",
      "Final error = 8.34e-08 \n",
      "Number of iterations = 24\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent \n",
    "xsol3, error3, count3 = steepestdescent(A,b,b,tol,maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a55b18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjugate Gradient (optional)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
